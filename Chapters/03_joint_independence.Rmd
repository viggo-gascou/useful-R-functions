---
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "pdfs") })

output:
  pdf_document:
    highlight: arrow
---

```{r "3_joint_independence-0", ECHO = FALSE, warning = FALSE, include = FALSE}
library(knitr)
library(kableExtra)
library(RColorBrewer)
library(UsingR)
library(ggplot2)
```

# Joint distributions and independence

A joint probability is as the name implies the probability of 'joining' the probability of two events i.e., $P(A \cap B)$ (the intersection of $A$ and $B$).
Take for example this table below:

```{r "3_joint_independence-1", echo=FALSE}
co <- matrix(c(1168, 825, 305, 573, 1312, 1200), byrow = TRUE, nrow = 2)
rownames(co) <- c("Light", "Dark")
colnames(co) <- c("Fair/Red", "Medium", "Dark/black")
dimnames(co) <- list("Eye color" = rownames(co), "Hair color" = colnames(co))

kable(co,
  caption = "Relation between hair color and eye color.", booktabs = T) %>%
  kable_styling() %>%
  add_header_above(c(" ", "Hair color" = 3), bold = T) %>%
  group_rows(index = c("Eye color" = 2))
```


To investigate the relations between hair color and eye colour, the hair color 
and eye color of 5383 was recorded. The data are given in Table 3. 
Eye color is encoded by the values 1 (Light) and 2 (Dark), 
and hair color by 1 (Fair/red), 2 (Medium), and 3 (Dark/black). 
By dividing the numbers in the table by 5383, the table is turned into a 
joint probability distribution for random variables X (hair color) 
taking values 1 to 3 and Y (eye color) taking values 1 and 2.


## Joint probability distribution



We determine the joint probability density distribution by dividing the eye and
hair color combinations by the total amount of records (5383) giving us this table (Table 4):
```{r "3_joint_independence-2"}
pmf <- matrix(round(c(1168, 825, 305, 573, 1312, 1200) / 5383, 3),
  byrow = TRUE, nrow = 2)
rownames(pmf) <- c("1 ", "2 ")
colnames(pmf) <- c("1", "2", "3")
dimnames(pmf) <- list("Y" = rownames(pmf), "X" = colnames(pmf))


kable(pmf,
  caption = "Joint probability distributions of X and Y", booktabs = T) %>%
  kable_styling(latex_options = "HOLD_position")
```

## Marginal probability distribution

In order to determine the marginal probability distribution of X and Y, we have to
marginalize for $Y$ and $X$ respectively which we do by finding $P(X = x)$ and $P(Y = y)$
which is done by summing all the values that $Y$ and $X$ can take. See table 5

```{r "3_joint_independence-3"}
marg <- rbind(pmf, colSums(pmf))
marg2 <- cbind(marg, rowSums(pmf))
rownames(marg2) <- c("1 ", "2 ", "Px ")
colnames(marg2) <- c("1", "2", "3", "Py ")
marg2[3, 4] <- 1

kable(marg2, format = "latex",
  caption = "Marginal probability distributions of X and Y", booktabs = T) %>%
  row_spec(2, extra_latex_after = "\\cline{1-5}") %>%
  column_spec(5, border_left = T) %>%
  kable_styling(latex_options = "HOLD_position")
```

## Independece between two variables

In order to find out whether $X$ and $Y$ are independent, we have to check that
$P(X = x, Y = y) = P(X = x) P(Y = y)$ for all $x$ and $y$. \
We'll start by checking for $x = 1$ and $y = 1$: \
$$
\begin{aligned}
P(X = 1, Y = 1) &= `r marg2[1,1]` \\
P(X = 1)P(Y = 1) &=  `r marg2[1,3]` \cdot `r marg2[3,1]` = `r marg2[1,3] * marg2[3,1]`
\end{aligned}
$$
Because $P(X = x, Y = y) \neq P(X = x) P(Y = y)$. We can say that $X$ and $Y$ are dependent
